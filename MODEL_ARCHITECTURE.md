# HSI Anomaly Detection Model Architecture

## Overview
This document provides a comprehensive breakdown of the Hyperspectral Imaging (HSI) anomaly detection system architecture, showing how HSI images are processed through the pipeline to identify and classify anomalies.

## System Components

### 1. Data Input Layer
- **Input Format**: MATLAB (.mat) files containing hyperspectral data cubes
- **Data Structure**: 3D tensors of shape (Height Ã— Width Ã— Spectral Bands)
- **Ground Truth**: Separate .mat files with pixel-level class labels

### 2. Preprocessing Pipeline
- **Noise Removal**: Dataset-specific band filtering (e.g., Indian Pines: removes bands 104-108, 150-163, 220)
- **Normalization**: MinMaxScaler normalization (values scaled to [0, 1])
- **Dimensionality Reduction**: PCA (Principal Component Analysis)
  - Pavia: 30 components
  - Indian/Salinas: 40 components
- **Output**: Reduced HSI cube (H Ã— W Ã— PCA_components)

### 3. Patch Extraction Module
- **Patch Size**: Configurable (default: 16Ã—16, classification uses 3Ã—3 or 5Ã—5)
- **Padding**: Reflect padding to handle boundary pixels
- **Spatial Context**: Extracts spatial-spectral patches maintaining spatial relationships
- **Output**: Flattened patches of shape (N_patches Ã— patch_sizeÂ² Ã— PCA_components)

### 4. Autoencoder Architecture

#### Encoder Network
```
Input: Flattened Patch (patch_sizeÂ² Ã— PCA_components)
  â†“
Linear Layer: (input_dim â†’ 512)
  â†“
ReLU Activation
  â†“
Linear Layer: (512 â†’ latent_dim)
  â†“
Output: Latent Representation Z (latent_dim = 32)
```

#### Decoder Network
```
Input: Latent Representation Z (latent_dim = 32)
  â†“
Linear Layer: (latent_dim â†’ 512)
  â†“
ReLU Activation
  â†“
Linear Layer: (512 â†’ input_dim)
  â†“
Output: Reconstructed Patch (same size as input)
```

**Loss Function**: Mean Squared Error (MSE) between input and reconstructed patches
**Training**: Adam optimizer (lr=0.001), Batch size=512, Early stopping (patience=3)

### 5. Latent Feature Extraction
- Encoder outputs compact 32-dimensional feature vectors
- Captures essential spectral-spatial patterns
- Used for both anomaly detection and classification

### 6. Transformer-based Anomaly Scoring

#### SimpleTransformer Architecture
```
Input: Latent Features Z (batch Ã— latent_dim)
  â†“
Unsqueeze: Add sequence dimension (batch Ã— 1 Ã— latent_dim)
  â†“
Multi-Head Self-Attention (4 heads, embed_dim=32)
  - Query: Z
  - Key: Z  
  - Value: Z
  â†“
Attention Output (batch Ã— 1 Ã— latent_dim)
  â†“
Squeeze: Remove sequence dimension (batch Ã— latent_dim)
  â†“
Linear Layer 1: (latent_dim â†’ 64)
  â†“
ReLU Activation
  â†“
Linear Layer 2: (64 â†’ 1)
  â†“
Output: Anomaly Scores (batch Ã— 1)
```

### 7. Classification Pipeline

#### SVM Classifier
- **Input**: PCA-reduced latent features (20 dimensions max)
- **Kernel**: RBF (Radial Basis Function)
- **Parameters**: C=5 (or 10 for classification pipeline), gamma='scale'
- **Class Balancing**: Automatic class weight computation
- **Output**: Class predictions and probabilities

### 8. Anomaly Detection Methods

#### Method 1: Reconstruction Error
- Compute MSE between input patches and reconstructed patches
- Threshold: 95th percentile of reconstruction errors
- Anomalies: Patches with error > threshold

#### Method 2: Transformer Scores
- Transformer outputs anomaly scores for each patch
- Higher scores indicate higher anomaly likelihood
- Normalized to [0, 1] range for visualization

### 9. Visualization & Output
- **Anomaly Heatmap**: Spatial distribution of anomaly scores
- **RGB Overlay**: Anomalies overlaid on PCA RGB visualization
- **Confusion Matrix**: Classification performance metrics
- **t-SNE Visualization**: 2D projection of latent space
- **Classification Reports**: Precision, Recall, F1-scores

---

## Detailed Mermaid Architecture Flowchart

## ðŸ§© 1. Data Preprocessing
```mermaid
flowchart TD
A["HSI Image Input\n(.mat file)\nHÃ—WÃ—Bands"] --> B["Load Data\nscipy.io.loadmat()"]
B --> C["Normalization\nScale data to [0,1]"]
C --> D["PCA Compression\nReduce spectral bands"]
D --> E["Patch Extraction\nNon-overlapping patches"]
E --> F["Preprocessed Data\nShape: NÃ—PCA_dim"]
```
## 2.Autoencoder Network
```mermaid
flowchart TD
A["Input Patches\nNÃ—input_dim"] --> B["Encoder\nLinear Layers + ReLU"]
B --> C["Latent Features Z\nNÃ—latent_dim"]
C --> D["Decoder\nLinear Layers + ReLU"]
D --> E["Reconstructed Output\nNÃ—input_dim"]
E --> F["Compute Reconstruction Loss\nMSE Loss + Adam Optimizer"]

```
## 3.Transformer + SVM Module
```mermaid
flowchart TD
A["Latent Features Z\nfrom Autoencoder"] --> B["Transformer Block\nSelf-Attention + Feed Forward"]
B --> C["Anomaly Score Map\nWeighted Attention Output"]
A --> D["SVM Classifier\nRBF Kernel"]
D --> E["Label Prediction\nNormal / Anomaly"]
C --> F["Combine Scores\nNormalize [0,1]"]
E --> G["Evaluation Metrics\nAccuracy, F1, ROC-AUC"]
```
## 4.Visualization & Output
```mermaid
flowchart TD
A["Anomaly Scores\n(HÃ—W Map)"] --> B["Reshape to Image"]
B --> C["Heatmap Visualization\nColormap: Inferno"]
C --> D["Overlay on RGB Image\nAlpha Blending"]
D --> E["Final Output\nSaved Results + Metrics"]
```

## Mathematical Formulations

### 1. Preprocessing
```
X_raw âˆˆ R^(HÃ—WÃ—B) â†’ X_norm âˆˆ [0,1]^(HÃ—WÃ—B)
X_norm = (X_raw - min(X_raw)) / (max(X_raw) - min(X_raw))

X_pca = PCA(X_norm, n_components) âˆˆ R^(HÃ—WÃ—D)
where D = 30 (Pavia) or 40 (Indian/Salinas)
```

### 2. Patch Extraction
```
For each pixel (i, j):
  Patch(i,j) = X_pca[i-margin:i+margin+1, j-margin:j+margin+1, :]
  Flatten: patch âˆˆ R^(patch_sizeÂ² Ã— D)
  
Input dimension: input_dim = patch_sizeÂ² Ã— D
```

### 3. Encoder Forward Pass
```
hâ‚ = ReLU(X Â· Wâ‚ + bâ‚)  âˆˆ R^512
Z = hâ‚ Â· Wâ‚‚ + bâ‚‚  âˆˆ R^32
```

### 4. Decoder Forward Pass
```
hâ‚‚ = ReLU(Z Â· Wâ‚ƒ + bâ‚ƒ)  âˆˆ R^512
XÌ‚ = hâ‚‚ Â· Wâ‚„ + bâ‚„  âˆˆ R^input_dim
```

### 5. Loss Function
```
L_MSE = (1/N) Î£áµ¢ ||Xáµ¢ - XÌ‚áµ¢||Â²
where N is batch size
```

### 6. Transformer Attention
```
Attention(Q, K, V) = softmax(QKáµ€/âˆšd_k) Â· V
where Q = ZÂ·W_q, K = ZÂ·W_k, V = ZÂ·W_v, d_k = 32
```

### 7. Anomaly Score
```
Score = Linearâ‚‚(ReLU(Linearâ‚(Attention(Z))))
Anomaly_Map[i,j] = Score[coordsâ»Â¹(i,j)]
```

### 8. Reconstruction Error
```
Error = (1/D) Î£â±¼ (X[j] - XÌ‚[j])Â²
Anomaly = Error > threshold_95
```

## Key Hyperparameters

| Parameter | Value | Description |
|-----------|-------|-------------|
| `patch_size` | 16 (default), 3-5 (classification) | Spatial context window size |
| `latent_dim` | 32 | Dimensionality of latent space |
| `encoder_hidden` | 512 | Hidden layer size in encoder/decoder |
| `pca_components` | 30 (Pavia), 40 (Indian/Salinas) | PCA dimensionality |
| `batch_size` | 512 | Training batch size |
| `learning_rate` | 0.001 | Adam optimizer learning rate |
| `num_epochs` | 10-20 | Maximum training epochs |
| `early_stopping_patience` | 3 | Early stopping patience |
| `transformer_heads` | 4 | Number of attention heads |
| `svm_C` | 5-10 | SVM regularization parameter |
| `svm_kernel` | 'rbf' | SVM kernel type |
| `anomaly_threshold` | 95th percentile | Reconstruction error threshold |

## Data Flow Dimensions

```
Input: H Ã— W Ã— Bands (e.g., 610 Ã— 340 Ã— 103 for Indian Pines)
  â†“
After PCA: H Ã— W Ã— D (D = 30 or 40)
  â†“
After Patch Extraction: N Ã— patch_size Ã— patch_size Ã— D
  â†“
After Flattening: N Ã— (patch_sizeÂ² Ã— D)
  â†“
Encoder Output: N Ã— 32 (latent features)
  â†“
Decoder Output: N Ã— (patch_sizeÂ² Ã— D) (reconstructed)
  â†“
Transformer Output: N Ã— 1 (anomaly scores)
  â†“
Anomaly Map: H Ã— W (spatial anomaly distribution)
```

## Performance Metrics

The system outputs:
- **Classification Accuracy**: Overall correct predictions
- **Per-Class Metrics**: Precision, Recall, F1-score for each class
- **AUC-ROC**: Area under ROC curve (if applicable)
- **Average Precision**: Macro-averaged precision
- **Confusion Matrix**: Detailed classification breakdown
- **Anomaly Detection Rate**: Percentage of anomalies identified
- **Spatial Distribution**: Visual heatmaps showing anomaly locations

